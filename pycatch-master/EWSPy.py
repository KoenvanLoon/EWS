# SPDX-License-Identifier: GPL-3.0-or-later
#
# Copyright (c) 2026 Koen van Loon
#
# See the LICENSE file in the repository root for full license text.

"""
EWS - Early Warning Signals
EWS Python functions (EWSPy)

@authors: KoenvanLoon & TijmenJanssen
"""

import numpy as np
import scipy.stats
import warnings
from scipy.signal import convolve
from statsmodels.api import OLS, add_constant
from statsmodels.tsa.ar_model import AutoReg

# import sys
# sys.path.append("./pcrasterModules/")

# File name as string
"""
File name as string of abbreviation + time 

    ! - This function is tailored for names generated by EWS_pycatch_hourly/weekly.py . If your inputs are from a 
    different source file, you can name the files in the same manner or swap this function out.
    
Args:
-----

name : Abbreviation of the variable name, e.g. 'bioA' for biomass average and 'bioM' for biomass map (spatial data).

timestep : The timestep corresponding to the modelled time step at which the file was created, for average timeseries
    this always corresponds to the final timestep, whereas spatial data is saved at regular intervals defined in the
    configuration.
    
Returns:
--------

file_name_str : File name as a string as generated by the pycatch models, e.g. bioM0104.000 for biomass map 'bioM' at
    timestep 104000 .
    
"""


def file_name_str(name, timestep):
    s = f"{int(timestep):07d}"
    return f"{name}{s[:-3]}.{s[-3:]}"


# Generated number length
"""
Generates the number length used for folder-names where generated data is stored. 

Args:
-----

realizations : int, number of generated datasets made.

Returns:
-----

generated_number_length : int, number length used for folder-names where generated data is stored. Standard is 4,
    otherwise number length equals the maximum number of individual numbers used

"""


def generated_number_length(realizations):
    return max(4, len(str(realizations)))


# Spatial early-warning signals
"""
Spatial methods included per phenomena

Rising memory :
    Spatial correlation : Moran's I

Rising variability :
    Spatial standard deviation 
    Spatial variance
    Spatial skewness
    
Other :
    Spatial kurtosis
    Power spectrum : Only works for square matrices - method not tested

"""

# Spatial mean
"""
Calculates the mean of 2D numpy arrays as found inside a 3D numpy array.

Args:
-----

fields : A 3D numpy array representing the temporal evolution of spatial (2D) data.

Returns:
-----

* : A 2D numpy array with the spatial mean over time. Calculated using np.nanmean().

"""


def spatial_mean(fields):
    return np.nanmean(fields, axis=(1, 2))


# Spatial standard deviation
"""
Calculates the standard deviation of 2D numpy arrays as found inside a 3D numpy array.

Args:
-----

fields : A 3D numpy array representing the temporal evolution of spatial (2D) data.

Returns:
-----

* : A 2D numpy array with the spatial standard deviation over time. Calculated using np.nanstd().

"""


def spatial_std(fields):
    return np.nanstd(fields, axis=(1, 2))


# Spatial variance
"""
Calculates the variance of 2D numpy arrays as found inside a 3D numpy array.

Args:
-----

fields : A 3D numpy array representing the temporal evolution of spatial (2D) data.

Returns:
-----

* : A 2D numpy array with the spatial variance over time. Calculated using np.nanvar().

"""


def spatial_var(fields):
    return np.nanvar(fields, axis=(1, 2))


# Spatial skewness
"""
Calculates the skewness of 2D numpy arrays as found inside a 3D numpy array.
NOTE: skewness and kurtosis are sensitive to missing spatial coverage -> trend scan reflect changing mask.

Args:
-----

fields : A 3D numpy array representing the temporal evolution of spatial (2D) data.

Returns:
-----

* : A 2D numpy array with the spatial skewness over time. Calculated using scipy.stats.skew().

"""


def spatial_skw(fields):
    return np.asarray([scipy.stats.skew(field.flatten(), nan_policy='omit') for field in fields])


# Spatial kurtosis
"""
Calculates the kurtosis of 2D numpy arrays as found inside a 3D numpy array.
NOTE: skewness and kurtosis are sensitive to missing spatial coverage -> trend scan reflect changing mask.

Args:
-----

fields : A 3D numpy array representing the temporal evolution of spatial (2D) data.

Returns:
-----

* : A 2D numpy array with the spatial kurtosis over time. Calculated using scipy.stats.kurtosis().

"""


def spatial_krt(fields):
    return np.asarray([scipy.stats.kurtosis(field.flatten(), nan_policy='omit') for field in fields])


# Spatial correlation (Moran's I)
"""
Calculates the spatial correlation of 2D numpy arrays as found inside a 3D numpy array.
(Normalized Moran's I adapted for masked grids with variable neighbour counts)
! Note: Assumes regular grid with equal cell spacing (both are true for PCRaster grids).

        N   sum_i( sum_j( w_i,j ( x_i - x_m ) ( x_j - x_m )
    I = - * -----------------------------------------------
        W               sum_i( x_i - x_m )^2
        
    Where :
        I : Moran's I, with -1 for perfect dispersion, 0 for perfect randomness, and 1 for perfect clustering
        N : Number of spatial units with (i,j)
        x : Variable of interest (e.g. biomass)
        x_m : The mean of x
        w_ij : A matrix of spatial weights
        W : The sum of all w_ij
        
    For spatial weights, the rook neighborhood is used. Hence, only the bordering spatial 'cells' are taken into
    consideration for calculating Moran's I. 

Args:
-----

fields : A 3D numpy array representing the temporal evolution of spatial (2D) data.

local : Boolean, True uses row-standardized spatial weights, yielding a Moran's I that behaves like a spatial AR(1) coeff.

Returns:
-----

* : A 2D numpy array with the spatial standard deviation over time. Calculated using Moran's I.

"""

rook_neighborhood = np.array([
    [0, 1, 0],
    [1, 0, 1],
    [0, 1, 0]
])

rook_neighborhood_local = np.array([
    [0, 1, 0],
    [1, 0, 1],
    [0, 1, 0]
], dtype=float)
rook_neighborhood_local /= np.sum(rook_neighborhood_local)


def spatial_corr(fields, local=False):  # Moran's I
    fields = np.asarray(fields)
    assert fields.ndim == 3, "Expected array of shape (time, x, y)"

    mean = spatial_mean(fields)
    mean = np.nan_to_num(mean, nan=0.0)

    fields_mmean = np.copy(fields)
    fields_mmean -= mean[:, None, None]

    is_nan = np.isnan(fields_mmean)  # missing values in map are assumed to be np.NaN
    is_not_nan = ~ is_nan
    is_not_nan_as_nr = is_not_nan.astype(float)

    fields_var = fields_mmean ** 2
    fields_var[is_nan] = 0.0

    fields_copy = np.copy(fields)
    fields_copy[is_nan] = 0.0

    if local is True:
        neighborhood = rook_neighborhood_local
    else:
        neighborhood = rook_neighborhood

    sum_neighbours = convolve(fields_copy, neighborhood[None, :, :], mode='same')

    n_neighbours = convolve(is_not_nan_as_nr, neighborhood[None, :, :], mode='same')
    n_neighbours_times_avg = convolve(is_not_nan_as_nr * mean[:, None, None], neighborhood[None, :, :], mode='same')
    n_neighbours_times_avg[is_nan] = 0.0

    P1 = np.nansum(fields_mmean * (sum_neighbours - n_neighbours_times_avg), axis=(1, 2))
    P2 = np.nansum(n_neighbours * fields_var, axis=(1, 2))

    N = np.nansum(is_not_nan_as_nr, axis=(1, 2))
    W = np.nansum(n_neighbours, axis=(1, 2))

    with np.errstate(divide='ignore', invalid='ignore'):
        I = (N / W) * (P1 / P2)

    return np.nan_to_num(I, nan=0.0)    # ! - Missing spatial coverage biases Moran's I toward zero (instead of var = 0 -> Moran's I undefined)
    # TODO test return np.NaN for nan=0.0


# Spatial DFT
"""
Spatial spectral EWS based on 2D DFT.
-> As a system approaches a tipping point, spatial patterns become smoother and dominated by large-scale structure;
    Large-scale structure = low spatial frequencies
-> Hence the question; What fraction of spatial variance is contained in low frequencies?

Args:
-----

fields : ndarray (T, X, Y)
    Time series of spatial snapshots.
low_freq_frac : float
    Fraction of lowest spatial frequencies used to compute low-frequency power.

Returns:
-----

dft_signal : ndarray (T,)
    Fraction of spectral power contained in low spatial frequencies.

"""


def spatial_DFT(fields, low_freq_frac=0.1):
    fields = np.asarray(fields)
    T = fields.shape[0]
    dft_signal = np.full(T, np.nan)

    for t in range(T):
        field = fields[t]

        if np.isnan(field).any():
            # ! - Skips invalid snapshots as FFTs do not handle NaNs
            continue

        F = np.fft.fft2(field)  # converts space to spatial frequency, patterns to wavelengths
        P = np.abs(F) ** 2      # Complex output -> power is proportional to variance at each frequency
        P = np.fft.fftshift(P)  # Centers low frequencies

        nx, ny = P.shape
        cx, cy = nx // 2, ny // 2
        r = int(low_freq_frac * min(nx, ny))    # creates central square round zero frequency;
                                                #   small r -> very large spatial scales
                                                #   large r -> finer patterns

        low_freq_power = np.sum(P[cx - r:cx + r + 1, cy - r:cy + r + 1])
        total_power = np.sum(P)

        if total_power > 0:
            dft_signal[t] = low_freq_power / total_power    # normalization, EWS(t) = sum low k P(K) / sum all k P(k)
                                                            # at tipping points; low-freq goes up, ratio goes up, spatial "reddening"

    return dft_signal


# spatial power spec
"""
Spatial power spectrum based early-warning signal.

Computes the slope of the radially averaged power spectrum for each spatial snapshot.
-> How does variance distribute across spatial scales?
    Compared to /how much/ low-freq power there is, we now ask; How fast does power decay with wavelength?

Args:
-----
fields : ndarray (T, X, Y)

Returns:
-----
ps_signal : ndarray (T,)
    Spectral slope of spatial power spectrum.
 """


def spatial_power_spec(fields):
    fields = np.asarray(fields)
    T = fields.shape[0]
    ps_signal = np.full(T, np.nan)

    for t in range(T):
        field = fields[t]

        if np.isnan(field).any():
            # Skip invalid snapshots as FFTs do not handle NaNs
            continue

        F = np.fft.fft2(field)  # Converts space to spatial frequency, patterns to wavelengths
        P = np.abs(F) ** 2      # Complex output -> power is proportional to variance at each frequency
        P = np.fft.fftshift(P)  # Center slow frequencies

        nx, ny = P.shape
        x = np.arange(-nx // 2, nx // 2)
        y = np.arange(-ny // 2, ny // 2)
        X, Y = np.meshgrid(x, y, indexing='ij')
        R = np.sqrt(X**2 + Y**2)    # Defines radial spatial frequency
                                    #   Small R -> large-scale patterns
                                    #   Large R -> fine grained noise

        r = R.astype(int)
        tbin = np.bincount(r.ravel(), P.ravel())
        nr = np.bincount(r.ravel())

        radial_power = tbin / np.maximum(nr, 1)

        k = np.arange(len(radial_power))
        valid = (radial_power > 0) & (k > 0)    # radial_power(r) = mean power of all pixels at radius r
                                                # Collapses 2D -> 1D; we now have Power vs spatial scale
        if np.sum(valid) > 2:
            coeffs = np.polyfit(np.log(k[valid]), np.log(radial_power[valid]), 1)   # log-log regression where we estimate k^a which is proportional to P(k) (a being the spectral slope)
            slope = coeffs[0]                                                       # flatter slope -> more large-scale dominance
            ps_signal[t] = slope

    return ps_signal


# Temporal early-warning signals
"""
Temporal methods included per phenomena

Rising memory :
    Autocorrelation at lag 1
    Autoregressive coefficient of AR(1)
    Return rate : Inverse of AR(1) coefficient
    DFA : Detrended Fluctuation Analysis

Rising variability & flickering :
    Standard deviation
    Variance
    Coefficient of variation : standard deviation / mean
    Skewness
    Kurtosis
    Conditional heteroskedasticity

"""


# Temporal AR(1)
"""
Calculates the AR(1) parameters of a 1D array (window) inside a 2D array (stack of windows).
Assumes stationarity and zero-mean innovations.
! Note: Assumes weak stationarity within each window.

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

Returns:
-----

AR1_params : Returns a 1D array containing the AR(1) parameter for each window calculated with 
    statsmodels.tsa_ar_model.AutoReg().

"""


def temporal_AR1(windows):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"

    mean = temporal_mean(windows)
    mean = np.nan_to_num(mean, nan=0.0)

    windows_mmean = np.copy(windows)
    windows_mmean -= mean[:, None]

    AR1_params = []

    for window in windows_mmean:
        try:
            mod = AutoReg(window, 1, trend='n').fit()
            AR1_params.append(mod.params[0])
        except Exception:
            AR1_params.append(np.nan)

    return np.asarray(AR1_params)


# Temporal return rate
"""
Calculates the returnrate of a 1D array (window) inside a 2D array (stack of windows).

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

Returns:
-----

* : Returns a 1D array containing the inverse of the AR(1) parameter for each window calculated with 
    statsmodels.tsa_ar_model.AutoReg().

"""


def temporal_returnrate(windows):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"

    ar1 = temporal_AR1(windows)
    return np.divide(1.0, ar1, out=np.full_like(ar1, np.nan, dtype=float), where=ar1 != 0)  # only true in linear discrete approximation.


# Temporal conditional heteroskedasticity
"""
Returns the F-value or the R-squared value with their associated p-value of the residuals of an AutoReg model fitted on
    a 1D array (window) inside a 2D array (stack of windows). McLeod-Li-style test.
    
    ! Note that the returned p-value corresponds to a X^2-based critical value, rather than an exact p-value.
    
    McLeodâ€“Li-style test for conditional heteroskedasticity; Detects temporal dependence in variance after removing linear autocorrelation.
    ! Used as a trend-based early warning signal, not a formal hypothesis test.

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

method : Selects whether 'R-squared' or 'F-statistic' is used as test of significance on the AutoReg residuals. 

alpha : Only called upon when method=='R-squared'. Selects used p-value.

log_transform : Selects whether the data is log-transformed before calculation. Usually False

Returns:
-----

test_statistic : The value of 'R-squared' or 'F-statistic'

crit_val : The critical value accompanying the test_statistic

"""


def temporal_cond_het(windows, method='R-squared', alpha=0.1, log_transform=False):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"

    if log_transform:
        windows = np.log10(windows)

    mean = temporal_mean(windows)
    mean = np.nan_to_num(mean, nan=0.0)
    windows_mmean = np.copy(windows)
    windows_mmean -= mean[:, None]

    test_statistic = []
    crit_val = []
    for k, window in enumerate(windows_mmean):
        ar_model = AutoReg(window, 1, trend='n').fit()
        # At runtime, type(ar_model.resid) == np.ndarray, so:
        # noinspection PyTypeChecker
        ar_resid_sq = ar_model.resid**2

        if np.nanvar(ar_resid_sq) == 0:
            test_statistic = np.append(test_statistic, np.nan)
            crit_val = np.append(crit_val, np.nan)
            continue

        X = add_constant(ar_resid_sq[:-1])
        lin_model = OLS(ar_resid_sq[1:], X).fit()  # t+1 is dependent on t
        # If significant (p-value lower than given value), heteroscedasticity is present.
        # At runtime, lin_model.fvalue & lin_model.pvalue are treated as floats, hence the noinspection PyTypeChecker.
        if method == 'F-statistic':
            # noinspection PyTypeChecker
            test_statistic = np.append(test_statistic, lin_model.fvalue)
            # noinspection PyTypeChecker
            crit_val = np.append(crit_val, lin_model.f_pvalue)
        elif method == 'R-squared':
            # noinspection PyTypeChecker
            test_statistic = np.append(test_statistic, lin_model.rsquared)
            crit_val = np.append(crit_val, scipy.stats.chi2.ppf((1 - alpha), df=1) / (len(ar_resid_sq)-1))

    return np.asarray(test_statistic), np.asarray(crit_val)


# Temporal autocorrelation
"""
Calculates the autocorrelation (correlation of a signal with a delayed copy of itself) at a specified lag of a 1D array 
    (window) inside a 2D array (stack of windows). 
    Note: Uses biased autocovariance estimator; normalization cancels in autocorrelation.

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

lag : Time lag, order. Usually set to 1

Returns:
-----

* : Returns a 1D array containing autocorrelation for a given lag for each window calculated by dividing the 
    autocovariance with the temporal variation.

"""


def temporal_autocorrelation(windows, lag=1):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"

    cov = temporal_autocovariance(windows, lag=lag)
    var = temporal_var(windows)

    return np.divide(cov, var, out=np.full_like(var, np.nan, dtype=float), where=var != 0)


# Temporal autocovariance
"""
Calculates the autocovariance (the covariance of the process with itself at pairs of time points) of a 1D array 
    (window) inside a 2D array (stack of windows).
    Note: biased estimator 1/N, sufficient for trend-based EWS analysis.

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

lag : Time lag, order. Usually set to 1

Returns:
-----

* : Returns a 1D array containing value of autocovariance for each window calculated for the specified lag.

"""


def temporal_autocovariance(windows, lag=1):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"

    auto_cov = np.full(len(windows), np.nan)
    for k, window in enumerate(windows):
        mean = np.nanmean(window)
        x = window[:-lag] - mean
        y = window[lag:] - mean

        mask = np.isfinite(x) & np.isfinite(y)
        if np.any(mask):
            auto_cov[k] = np.mean(x[mask] * y[mask])

    return auto_cov


# Temporal mean
"""
Calculates the mean of 1D numpy array (window) as found inside a 2D numpy array (stack of windows).

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

Returns:
-----

* : A 1D numpy array with the temporal mean over time for each window. Calculated using np.nanmean().

"""


def temporal_mean(windows):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"
    return np.nanmean(windows, axis=1)


# Temporal standard deviation
"""
Calculates the standard deviation of 1D numpy array (window) as found inside a 2D numpy array (stack of windows).

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

Returns:
-----

* : A 1D numpy array with the temporal standard deviation over time for each window. Calculated using np.nanstd().

"""


def temporal_std(windows):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"
    return np.nanstd(windows, axis=1)


# Temporal variance
"""
Calculates the variance of 1D numpy array (window) as found inside a 2D numpy array (stack of windows).

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

Returns:
-----

* : A 1D numpy array with the temporal variance over time for each window. Calculated using np.nanvar().

"""


def temporal_var(windows):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"
    return np.nanvar(windows, axis=1)


# Temporal coefficient of variation
"""
Calculates the coefficient of variation of 1D numpy array (window) as found inside a 2D numpy array (stack of windows).

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

Returns:
-----

* : A 1D numpy array with the temporal coefficient of variation over time for each window. Calculated by dividing the
    temporal standard deviation with the mean.

"""


def temporal_cv(windows):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"

    mean = temporal_mean(windows)
    std = temporal_std(windows)

    return np.divide(std, mean, out=np.full_like(mean, np.nan, dtype=float), where=mean != 0)


# Temporal skewness
"""
Calculates the skewness of 1D numpy array (window) as found inside a 2D numpy array (stack of windows).

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

Returns:
-----

* : A 1D numpy array with the temporal skewness over time for each window. Calculated using scipy.stats.skew().

"""


def temporal_skw(windows):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"
    return scipy.stats.skew(windows, axis=1, nan_policy='omit')


# Temporal kurtosis
"""
Calculates the kurtosis of 1D numpy array (window) as found inside a 2D numpy array (stack of windows).

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

Returns:
-----

* : A 1D numpy array with the temporal kurtosis over time for each window. Calculated using scipy.stats.kurtosis().

"""


def temporal_krt(windows):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"
    return scipy.stats.kurtosis(windows, axis=1, nan_policy='omit')


# Divisor generator
"""
Calculates the scales over which fluctuations are calculated for Detrended Fluctuation Analysis (DFA). This function
    ensures that no remainders exist after division.

Args:
-----

lower_limit : The minimum amount of datapoints available for a scale used in DFA. Usually set to 10.

upper_limit : The maximum amount of datapoints available for a scale used in DFA. Usually equal to the window size.

Returns:
-----

* : A 1D numpy array with sorted scales.

"""


def divisor_generator(lower_limit, upper_limit):
    divisors = set()
    for i in range(1, int(np.sqrt(upper_limit)) + 1):
        if upper_limit % i == 0:
            divisors.add(i)
            divisors.add(upper_limit // i)
    return np.array(sorted(d for d in divisors if d >= lower_limit))


# Windowed Root Mean Square (RMS) with linear detrending
"""
Calculates the mean of 1D numpy array (window) as found inside a 2D numpy array (stack of windows).

Args:
-----

numpy_array : A 1D numpy array containing a window from a sliced timeseries (stack of windows).

scale : The number of datapoints in each segment into which the window is divided.

Returns:
-----

rms : A 1D numpy array with the quadratic mean for each scale (segment) of a given window.

"""


def calc_rms(numpy_array, scale):
    # Making of an array with data divided into segments
    n = len(numpy_array) // scale
    segments = numpy_array[:n*scale].reshape(n, scale)

    # Vector of x-axis
    scale_ax = np.arange(scale)
    rms = np.zeros(segments.shape[0])
    for i, segment in enumerate(segments):
        coeff = np.polyfit(scale_ax, segment, 1)
        xfit = np.polyval(coeff, scale_ax)
        # Detrending & computing RMS of each window
        rms[i] = np.sqrt(np.nanmean((segment - xfit)**2))
    return rms


# Detrended Fluctuation Analysis Propagator
"""
Calculates the propagator used for Detrended Fluctuation Analysis (DFA).

    ! - This is an empirical equation, and should be calibrated (polynomial regression) for *every* state variable 
    separately, which requires an AR(1) generated dataset based on the original dataset (see temporal null models).
    Furthermore, this adds all the assumptions of AR(1) as it translates the coefficient to the propagator (which is 
    assumed to be, but is not necessarily between 0 and 1) and can also introduce a certain error due to calibration 
    with (a non-optimal) polynomial regression.

Args:
-----

alpha : Polynomial coefficient taken as constant.

c_guess : Initial guess for c-value.

Returns:
-----

x1 : C-value after 5 iterations.

"""


def dfa_propagator(alpha, c_guess=0.5):
    # Note that this introduces AR(1)-based assumption indirectly.
    # 0.91 * (c ** 3) - 0.37 * (c ** 2) + 0.49 * c + c - alpha = 0
    # a*x**3 + b*x**2 + c*x + d = 0.

    x1 = c_guess
    count = 0
    while count < 5:

        # if 0 < x1 <= 0.936:
        if x1 <= 0.936:
            a = 0.91
            b = -0.37
            c = 0.49
            d = 0.52 - alpha

        elif 0.936 < x1 <= 0.967:
            a = 0
            b = -12.38
            c = 25.14
            d = 11.28 - alpha

        # elif 0.967 < x1 < 1:  # x1 can be greater than 1
        elif 0.967 < x1:
            a = 0
            b = 0
            c = 0.72
            d = 0.75 - alpha

        else:
            assert False, "Floating point arithmetic went wrong in DFA propagator."

        # Constructs the polynomial a*x**3 + b*x**2 + c*x + d
        poly = np.poly1d([a, b, c, d])
        roots = np.roots(poly)
        real_roots = roots[np.isreal(roots)].real

        if len(real_roots) == 0:
            return np.nan

        x1 = real_roots[np.argmin(np.abs(real_roots - x1))]

        count += 1

    return x1


# Temporal Detrended Fluctuation Analysis (DFA)
"""
Calculates the temporal detrended fluctuation analysis of 1D numpy array (window) as found inside a 2D numpy array 
    (stack of windows).

Args:
-----

windows : A 2D numpy array containing a sliced timeseries (stack of windows).

Returns:
-----

scales : Scales used for DFA calculations. See Divisor generator

fluct : Fluctuations; the absolute mean of the root mean square. See calc_rms.

coeff : Coefficients of np.polyfit() for scales and fluctuations.

propagator : If not return_propagater; equal to coeff. Otherwise, dfa_propagator is used to calculate C-value 
    (see dfa_propagator).

"""


def temporal_dfa(windows, return_propagator=False, return_all=False):
    assert windows.ndim == 2, "Expected array of shape (windows, time)"

    fluct = []
    coeff = []
    window_size = windows.shape[1]
    scales = divisor_generator(10, window_size)
    # Note that, if window_size is a prime number, the only divisors are 1 and itself.
    #   With filtering >=10, it's possibly that DFA returns empty.
    if len(scales) == 0:
        warnings.warn(
            f"DFA cannot be computed for window_size={window_size}.\n"
            f"No divisors >= 10 found (window size might be prime or too small."
        )
    propagator = []

    for window in windows:
        # Cumulative sum of a single window with subtracted offset
        y = np.nancumsum(window - np.nanmean(window))  # noise like time series to random walk time series

        # RMS for each segment
        fluctuations = np.zeros(len(scales))  # a.ii.1
        for i, sc in enumerate(scales):
            fluctuations[i] = np.sqrt(np.mean(calc_rms(y, sc)**2))

        valid = (fluctuations > 0) & np.isfinite(fluctuations)

        if np.sum(valid) > 2:
            coefficients = np.polyfit(np.log2(scales[valid]), np.log2(fluctuations[valid]), 1)
        else:
            coefficients = [np.nan, np.nan]

        fluct.append(fluctuations)
        coeff.append(coefficients[0])

        if return_propagator:
            propagator.append(dfa_propagator(coefficients[0]))

    fluct = np.array(fluct)
    coeff = np.array(coeff)
    propagator = np.array(propagator) if return_propagator else coeff

    if return_all:
        return scales, fluct, coeff, propagator

    return propagator
